{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f6ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(10)\n",
    "from keras.datasets import mnist\n",
    "(X_train_image, y_train_label),\\\n",
    "(X_test_image, y_test_label) = mnist.load_data()\n",
    "x_Train = X_train_image.reshape(60000,784).astype('float32')\n",
    "x_Test = X_test_image.reshape(10000,784).astype('float32')\n",
    "x_Train_normalized = x_Train/225\n",
    "x_Test_normalized = x_Test/225\n",
    "y_TrainOneHot = np_utils.to_categorical(y_train_label)\n",
    "y_TestOneHot = np_utils.to_categorical(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba292242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_image(image):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(5,5)\n",
    "    plt.imshow(image,cmap='binary')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9266371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "class CNN:\n",
    "    def __init__(self,layers,Clayers):\n",
    "        self.layers = layers\n",
    "        self.convolution = 1\n",
    "        self.nums = len(layers)\n",
    "        self.result = []\n",
    "        self.weight = [np.random.randn(x,y) for x,y in zip(layers[1:],layers)]\n",
    "        self.bias = [np.random.randn(p) for p in layers[1:]]\n",
    "        self.loss = []\n",
    "        self.data_a = [np.ones((x)) for x in layers]\n",
    "        self.data_z = [np.ones((x)) for x in layers[1:]]\n",
    "        self.kernel = [np.random.randn(x,y,3,3) for x,y in zip(Clayers[1:],Clayers)]\n",
    "        self.convol_bias = [np.zeros((x)) for x in Clayers[1:]]\n",
    "        self.data_convol = []\n",
    "        self.data_convol_z = []\n",
    "        self.data_pooling = []\n",
    "    def feedforward(self):\n",
    "        self.data_a[0] = np.array(self.data_pooling[len(self.data_pooling)-1])\n",
    "        temp_reform = (self.data_a[0].shape)\n",
    "        self.data_a[0] = self.data_a[0].reshape(temp_reform[0]*temp_reform[1]*temp_reform[2])\n",
    "        for i in range (self.nums-1): \n",
    "            z = np.matmul(self.weight[i],self.data_a[i])\n",
    "            z += self.bias[i]\n",
    "            temp = copy.copy(z)\n",
    "            self.data_z[i] = temp\n",
    "            #z = ReLU_FC(z)\n",
    "            z = sigmoid(z)\n",
    "            self.data_a[i+1] = z\n",
    "        self.result = self.data_a[self.nums-1]\n",
    "    def backpropagation(self,correction,learning_rate):\n",
    "        C0_deri = (self.result-correction)*2\n",
    "        C0_deri = np.array(C0_deri)\n",
    "        self.loss.append(np.sum(np.square(self.result-correction)))\n",
    "        for i in range(self.nums-2,-1,-1):\n",
    "            #sigmoid = np.array(ReLU_deri_FC(self.data_z[i]),dtype='float64')\n",
    "            sigmoid = np.array(sigmoid_der(self.data_z[i]),dtype='float64')\n",
    "            sigmoid = sigmoid*C0_deri\n",
    "            temp = np.array([sigmoid])\n",
    "            C0_deri = np.matmul(self.weight[i].T,sigmoid)\n",
    "            weight = np.matmul(np.transpose(temp),np.array([self.data_a[i]]))\n",
    "            self.learning(i,np.array(weight),np.array(sigmoid),learning_rate,correction)\n",
    "        self.backprop_nonFC(C0_deri.reshape(12,13,13),self.convolution-1,learning_rate)\n",
    "    def convolutionLayer(self,input_):\n",
    "        self.data_pooling.append([input_])\n",
    "        for i in range(self.convolution):\n",
    "            output = np.zeros((int(len(self.kernel[i])),len(self.data_pooling[i][0])-2,len(self.data_pooling[i][0])-2))\n",
    "            for j in range(len(self.kernel[i])):\n",
    "                sum = np.zeros((len(self.data_pooling[i][0])-2,len(self.data_pooling[i][0])-2))\n",
    "                for k in range(len(self.kernel[i][0])):\n",
    "                    sum+=convolute(self.data_pooling[i][k],self.kernel[i][j][k])\n",
    "                output[j]=sum+self.convol_bias[i][j]\n",
    "            self.data_convol_z.append(copy.copy(output))\n",
    "            temp = sigmoid(output)\n",
    "            #temp = ReLU(output)\n",
    "            self.data_pooling.append(average_pooling(temp))\n",
    "    def backprop_nonFC(self,input_deri,position,learning_rate):\n",
    "        if(position<0):\n",
    "            self.data_convol.clear()\n",
    "            self.data_convol_z.clear()\n",
    "            self.data_pooling.clear()\n",
    "            return 0\n",
    "        C0_deri = []\n",
    "        for j in range (len(input_deri)):\n",
    "            C0_deri.append(backprop_pooling(input_deri[j]))\n",
    "        #C0_F = C0_deri*ReLU_deri(self.data_convol_z[position])\n",
    "        C0_F = C0_deri*sigmoid_der(self.data_convol_z[position])\n",
    "        bias_deri = []\n",
    "        for i in range(len(C0_F)):\n",
    "            bias_deri.append(np.sum(C0_F[i]))\n",
    "        self.learningBias(position,bias_deri,learning_rate)\n",
    "        for i in range(len(C0_F)):\n",
    "            for j in range(len(self.data_pooling[position])):\n",
    "                C0_k = convolute(self.data_pooling[position][j],C0_F[i])\n",
    "                self.learningConvol(position,i,j,C0_k,learning_rate)\n",
    "        C0_next=[]\n",
    "        for i in range(len(self.data_pooling[position])):\n",
    "            sum = np.zeros((len(self.data_pooling[position][0]),len(self.data_pooling[position][0])))\n",
    "            for j in range(len(self.kernel[position])):\n",
    "                sum+=full_convolute(self.kernel[position][j][i],C0_F[j])\n",
    "            C0_next.append(sum)\n",
    "        self.backprop_nonFC(C0_next,position-1,learning_rate)\n",
    "    def learning(self,i,weight,bias,learning_rate,correction):\n",
    "        n = -1*learning_rate\n",
    "        delta_bias = n*bias\n",
    "        delta_weight = n*weight\n",
    "        self.weight[i]+=delta_weight\n",
    "        self.bias[i]+=delta_bias\n",
    "    def learningConvol(self,i,j,k,kernel,learning_rate):\n",
    "        print(kernel)\n",
    "        n = -1*learning_rate\n",
    "        delta_kernel = n*kernel\n",
    "        self.kernel[i][j][k] += delta_kernel\n",
    "    def learningBias(self,i,bias_deri,learning_rate):\n",
    "        n = -1*learning_rate\n",
    "        delta_kernel = np.array(bias_deri)\n",
    "        delta_kernel = delta_kernel*n\n",
    "        self.convol_bias[i]+=delta_kernel\n",
    "def average_pooling(a):\n",
    "    output = []\n",
    "    for i in range(len(a)):\n",
    "        s = np.zeros((int(len(a[i])/2),int(len(a[i])/2)))\n",
    "        for j in range(len(a[i])):\n",
    "            for k in range(len(a[i])):\n",
    "                s[int(j/2),int(k/2)]+=a[i][j][k]/4\n",
    "        output.append(s)\n",
    "    return output\n",
    "def backprop_pooling(C0):\n",
    "    output = np.zeros((len(C0)*2,len(C0)*2))\n",
    "    for i in range(len(C0)*2):\n",
    "        for j in range(len(C0)*2):\n",
    "            output[i][j] = C0[int(i/2)][int(j/2)]*1/4\n",
    "    return output\n",
    "def convolute(a,b):\n",
    "    output = []\n",
    "    for i in range(len(a)-len(b)+1):\n",
    "        temp = []\n",
    "        for j in range(len(a[0])-len(b)+1):\n",
    "            sum_ = 0\n",
    "            for k in range(len(b)):                    \n",
    "                sum_ += np.dot(a[i+k][j:j+len(b[k])],b[k])\n",
    "            temp.append(sum_)\n",
    "        output.append(temp)\n",
    "    return np.array(output)\n",
    "def full_convolute(a,b):\n",
    "    rotate = np.zeros((len(b),len(b)))\n",
    "    for i in range(len(b)):\n",
    "        rotate[i][len(b)-i-1] = 1\n",
    "    b = np.matmul(rotate,b)\n",
    "    wrap = np.pad(a, len(b)-1, pad_with)\n",
    "    return convolute(wrap,b)\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-1*z))\n",
    "def sigmoid_der(z):\n",
    "    a = sigmoid(z)\n",
    "    return a*(1-a)\n",
    "def ReLU(z):\n",
    "    for k in range(len(z)):\n",
    "        for i in range(len(z[0])):\n",
    "            for j in range(len(z[0][0])):\n",
    "                if(z[k][i][j]<0):\n",
    "                    z[k][i][j]=max(0,z[k][i][j])\n",
    "    return z\n",
    "def ReLU_deri(z):\n",
    "    output = np.zeros((len(z),len(z[0]),len(z[0][0])))\n",
    "    for k in range(len(z)):\n",
    "        for i in range(len(z[0])):\n",
    "            for j in range(len(z[0][0])):\n",
    "                if(z[k][i][j]>=0):\n",
    "                    output[k][i][j] = 1\n",
    "                else:\n",
    "                    output[k][i][j] = 0\n",
    "    return output\n",
    "def ReLU_FC(z):\n",
    "    c = np.zeros((len(z)))\n",
    "    for i in range(len(z)):\n",
    "        if(z[i]<0):\n",
    "            c[i]=z[i]*0.01\n",
    "        else:\n",
    "            c[i]=z[i]\n",
    "    return c\n",
    "def ReLU_deri_FC(z):\n",
    "    c = np.zeros((len(z)))\n",
    "    for i in range(len(z)):\n",
    "        if(z[i]>0):\n",
    "            c[i] = 1\n",
    "        else:\n",
    "            c[i] = 0.01\n",
    "    return c\n",
    "def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "    pad_value = kwargs.get('padder', 0)\n",
    "    vector[:pad_width[0]] = pad_value\n",
    "    vector[-pad_width[1]:] = pad_value\n",
    "network = CNN([2028,400,10],[1,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997953da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "a = []\n",
    "b = []\n",
    "network.loss.clear()\n",
    "for p in range(30):\n",
    "    b.append(p)\n",
    "    sum_value = 0\n",
    "    for i in range (100):\n",
    "        network.convolutionLayer(x_Train_normalized[100*p+i].reshape(28,28))\n",
    "        network.feedforward()\n",
    "        index = np.argmax(network.result)\n",
    "        if(index==y_train_label[100*p+i]):\n",
    "            sum_value+=1     \n",
    "        network.backpropagation(y_TrainOneHot[100*p+i],0.25)\n",
    "    print(\"epoch \",p+1)\n",
    "    print(sum_value,\"/100\")\n",
    "    print(\"accuracy\",sum_value,\"%\")\n",
    "    a.append(np.sum(network.loss))\n",
    "    network.loss.clear()\n",
    "    sum_value=0\n",
    "    for i in range(100):\n",
    "        network.convolutionLayer(x_Test_normalized[i].reshape(28,28))\n",
    "        network.feedforward()\n",
    "        index = np.argmax(network.result)\n",
    "        if(index==y_test_label[i]):\n",
    "            sum_value+=1   \n",
    "    print(sum_value,\"/100\")\n",
    "    print(\"accuracy\",sum_value,\"%\")\n",
    "plt.plot(b,a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
